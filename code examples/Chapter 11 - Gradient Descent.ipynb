{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most plain implementation of gradient descent, for minimizing a differentiable function $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaGradientDescent(f, f_grad, init=np.random.uniform(-1, 1, 2), eta=lambda t: .1, tol=1e-5):\n",
    "    steps, delta = [init], tol\n",
    "\n",
    "    t = 1\n",
    "    while delta >= tol:\n",
    "        g, eta_t = f_grad(steps[-1]), eta(t)\n",
    "        step = steps[-1] - eta_t * g\n",
    "        \n",
    "        steps.append(step)\n",
    "        delta = np.sum((steps[-1] - steps[-2])**2)**.5\n",
    "        t += 1\n",
    "        \n",
    "    return np.array(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used for plotting (in 2D and 3D) the loss surface of a given function to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_array(x):\n",
    "    return np.array([x]) if np.isscalar(x) else x\n",
    "\n",
    "def function_contour(fun, vals):\n",
    "    xx, yy = np.meshgrid(vals, vals)\n",
    "    z = fun(np.c_[xx.ravel(), yy.ravel()]).reshape(len(vals), len(vals))\n",
    "    return go.Contour(x = vals, y=vals, z=z, opacity=.4, colorscale=\"Blues_r\", showscale=False)\n",
    "\n",
    "def function_surface(fun, vals):\n",
    "    xx, yy = np.meshgrid(vals, vals)\n",
    "    z = fun(np.c_[xx.ravel(), yy.ravel()]).reshape(len(vals), len(vals))\n",
    "    return go.Surface(x = vals, y=vals, z=z, opacity=.4, colorscale=\"Blues_r\", showscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize RSS Using GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(X: np.ndarray, y: np.ndarray):\n",
    "    def _evaluate(w: np.ndarray):\n",
    "        Y = np.broadcast_to(y[..., np.newaxis], (y.shape[0], w.shape[0]))\n",
    "        return np.sum( (X @ w.T - Y)**2, axis=0)\n",
    "    \n",
    "    def _gradient(w: np.ndarray):\n",
    "        return 2 * X.T @ (X @ w.T - y)\n",
    "    \n",
    "    return _evaluate, _gradient\n",
    "\n",
    "\n",
    "n = 50\n",
    "w = np.random.random(size = (2, ))\n",
    "X = np.c_[np.random.uniform(low=-3, high=3, size=(n, 1)), np.ones((n, 1))]\n",
    "y = X @ w + np.random.normal(0, 1, size=(n,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RSS module above (the evaluation and gradient computation) functions above we explore the gradient descent algorithm. First, we can track the stepping of the algorithm in the parameter space (i.e. obaining different feasible solutions $\\mathbf{w}$ at each iteration) and observe the linear model it reflects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_grad = RSS(X, y)\n",
    "\n",
    "# Run the GD algorithm\n",
    "steps = VanillaGradientDescent(f, f_grad, \n",
    "                               init=np.array([4.5,-4]), \n",
    "                               eta=lambda t: .005,\n",
    "                               tol=5*1e-3)\n",
    "\n",
    "# Obtain objective surface\n",
    "vals = np.linspace(-5, 5, 50)\n",
    "contour = function_contour(f, vals)\n",
    "    \n",
    "frames, markers = [], []\n",
    "for i in range(1, len(steps)+1):\n",
    "    z = as_array(f(steps[:i]))\n",
    "    frames.append(go.Frame(data=[\n",
    "        # 2D visualization of progress\n",
    "        go.Scatter(x=steps[:i,0], y=steps[:i,1], marker=dict(size=3, color=\"black\"), showlegend=False),\n",
    "        go.Scatter(x=[steps[i-1,0]], y=[steps[i-1,1]], marker=dict(size=5, color=\"red\"), showlegend=False), \n",
    "        contour,\n",
    "\n",
    "        # Visualization of regression line and data\n",
    "        go.Scatter(x=X[:, 0], y=y, marker=dict(size=5, color=\"black\"), mode = 'markers', showlegend=False, xaxis=\"x2\", yaxis=\"y2\"),\n",
    "        go.Scatter(x=[X[:, 0].min(), X[:, 0].max()], \n",
    "                   y=[X[:, 0].min()*steps[i-1,0] + steps[i-1,1], X[:, 0].max()*steps[i-1,0] + steps[i-1,1]],\n",
    "                   marker=dict(size=3, color=\"Blue\"), mode='lines', showlegend=False, xaxis=\"x2\", yaxis=\"y2\")],\n",
    "        traces=[0, 1, 2, 3, 4, 5],\n",
    "        layout=go.Layout(title=rf\"$\\text{{Iteration }} {i}/{steps.shape[0]}$\" )))\n",
    "\n",
    "# Create animated figure\n",
    "fig = make_subplots(rows=1, cols=2, column_widths = [400, 700], horizontal_spacing=.075,\n",
    "                    subplot_titles=(r\"$\\text{RSS Loss Surface}$\", r\"$\\text{Fitted Model}$\"))\\\n",
    "    .update_layout(width=1100, height = 400, title = frames[0].layout.title,\n",
    "                   updatemenus = [dict(type=\"buttons\", buttons=[AnimationButtons.play(1200,0), \n",
    "                                                                AnimationButtons.pause()])])\n",
    "\n",
    "fig = fig.add_traces(frames[0][\"data\"], rows=1, cols=[1, 1, 1, 2, 2])\\\n",
    "         .update(frames = frames)\n",
    "\n",
    "fig = fig.update_xaxes(range=[vals[0], vals[-1]], title=r\"$\\text{Coefficient }w_1$\", col=1)\\\n",
    "         .update_yaxes(range=[vals[0], vals[-1]], title=r\"$\\text{Coefficient }w_2$\", col=1)\\\n",
    "         .update_xaxes(title=r\"$\\text{Variable } x$\", col=2)\\\n",
    "         .update_yaxes(range=[min(y)-.5, max(y)+.5], title=r\"$\\text{Response }y$\", col=2)\n",
    "    \n",
    "animation_to_gif(fig, \"../figures/rss_gd_opt.gif\", 700, width=1100, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examin the RSS optimization process for different constant values of the step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, f_grad = RSS(X, y)\n",
    "\n",
    "vals = np.linspace(-5, 5, 50)\n",
    "contour = function_contour(f, vals)\n",
    "\n",
    "eta = .0065\n",
    "steps = VanillaGradientDescent(f, f_grad, eta=lambda t: eta,tol = 1e-5, init=np.array([4.5,-4]))\n",
    "\n",
    "fig = go.Figure(data = \n",
    "          [go.Scatter(x=steps[:,0], y=steps[:,1], marker=dict(size=3, color=\"black\"), mode=\"markers+lines\", showlegend=False),\n",
    "           contour],\n",
    "          layout = go.Layout(\n",
    "              width=400, height=400,\n",
    "              xaxis = dict(title = r\"$\\text{Coefficient }w_1$\", range=[-5,5]),\n",
    "              yaxis = dict(title = r\"$\\text{Coefficient }w_2$\", range=[-5,5]),\n",
    "              title = rf\"$\\text{{Step Size: }}\\eta={eta} \\text{{ (}}n={len(steps)}\\text{{ Iterations)}}$\"\n",
    "          ))\n",
    "\n",
    "fig.write_image(f\"../figures/rss_gd_eta_{eta}.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2/3D Traverse In Parameter Space For GD Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Animate_GradientDescent(f, f_grad, init, eta, delta, axis_range, frame_time=500):    \n",
    "    steps = VanillaGradientDescent(f, f_grad, init, eta, delta)\n",
    "    surface, contour = function_surface(f, axis_range), function_contour(f, axis_range)\n",
    "    \n",
    "    frames, markers = [], []\n",
    "    for i in range(1, len(steps) + 1):\n",
    "        z = as_array(f(steps[:i]))       \n",
    "        frames.append(go.Frame(data=[\n",
    "            # 3D visualization of progress\n",
    "            go.Scatter3d(x=steps[:i,0], y=steps[:i,1], z=z[:i], marker=dict(size=3, color=\"black\"), showlegend=False),\n",
    "            go.Scatter3d(x=[steps[i-1,0]], y=[steps[i-1,1]], z=[z[i-1]],marker=dict(size=5, color=\"red\"), showlegend=False), \n",
    "            surface,\n",
    "            \n",
    "            # 2D visualization of progress\n",
    "            go.Scatter(x=steps[:i,0], y=steps[:i,1], marker=dict(size=3, color=\"black\"), mode=\"markers+lines\", showlegend=False),\n",
    "            go.Scatter(x=[steps[i-1,0]], y=[steps[i-1,1]], marker=dict(size=5, color=\"red\"), showlegend=False), \n",
    "            contour],\n",
    "            traces=[0, 1, 2, 3, 4, 5],\n",
    "            layout=go.Layout(title=rf\"$\\text{{Iteration }} {i}/{steps.shape[0]}$\" )))\n",
    "\n",
    "    return make_subplots(rows=1, cols=2, specs=[[{'type':'scene'}, {}]],\n",
    "                         subplot_titles=('3D Visualization Of Function', '2D Visualization Of Function'))\\\n",
    "        .add_traces(data=frames[0][\"data\"], rows=[1, 1, 1, 1, 1, 1], cols=[1, 1, 1, 2, 2, 2])\\\n",
    "        .update(frames = frames)\\\n",
    "        .update_xaxes(range=[axis_range[0], axis_range[-1]])\\\n",
    "        .update_yaxes(range=[axis_range[0], axis_range[-1]])\\\n",
    "        .update_layout(width=900, height = 330, title = frames[0].layout.title,\n",
    "                       updatemenus = [dict(type=\"buttons\", buttons=[AnimationButtons.play(frame_time,0), \n",
    "                                                                    AnimationButtons.pause()])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Over Gaussian Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve, det\n",
    "\n",
    "def negative_gaussian(mu=np.zeros(2), cov=np.eye(2)):\n",
    "    from scipy.stats import multivariate_normal\n",
    "    \n",
    "    def _evaluate(x: np.ndarray):\n",
    "        return  - multivariate_normal(mu, cov).pdf(x)\n",
    "\n",
    "    def _gradient(x: np.ndarray):\n",
    "        z = solve(cov,x-mu)\n",
    "        return np.exp(-z @ (x-mu) /2) * z / (2*np.sqrt((2*np.pi)**mu.shape[0] * det(cov)))\n",
    "    \n",
    "    return _evaluate, _gradient\n",
    "\n",
    "\n",
    "Animate_GradientDescent(*negative_gaussian(cov=[5,10]*np.eye(2)),\n",
    "                        init=np.array([-4.8,-4.8]), \n",
    "                        eta= lambda t: 300, \n",
    "                        delta=1e-2, \n",
    "                        axis_range=np.linspace(-5, 5, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_convex_function():\n",
    "    def _evaluate(x: np.ndarray):\n",
    "        x = np.stack(x, axis=0)\n",
    "        z = np.sin(x[:, 0] * x[:, 1]) / np.sqrt(x[:, 0]**2 + x[:, 1]**2)\n",
    "\n",
    "        return np.array([[z]]) if np.isscalar(z) else z\n",
    "\n",
    "    \n",
    "    def _gradient(x: np.ndarray):\n",
    "        X, Y = x[0], x[1]\n",
    "        a = np.array([(Y*np.cos(X*Y)*(X**2 + Y**2) - X*np.sin(X*Y)) / (X**2 + Y**2)**(1.5),\n",
    "                     (X*np.cos(X*Y)*(X**2 + Y**2) - Y*np.sin(X*Y)) / (X**2 + Y**2)**(1.5)])\n",
    "        return a\n",
    "    \n",
    "    return _evaluate, _gradient\n",
    "\n",
    "\n",
    "Animate_GradientDescent(*non_convex_function(),\n",
    "                        init=np.random.uniform(-5,5,2),\n",
    "                        eta= lambda t: 2*.1, \n",
    "                        delta=1e-3, \n",
    "                        axis_range=np.linspace(-5, 5, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Below is a naive implementation of the stochastic gradient descent, recieving a \"module\" to minimize and a batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaStochasticGradientDescent(module, init=np.random.uniform(-1, 1, 2), eta=lambda t: .1, tol=1e-5, batch_size=5):\n",
    "    steps, delta = [init], tol\n",
    "\n",
    "    t = 1\n",
    "    while delta >= tol:\n",
    "        # Sample data for current iteration\n",
    "        ids = module.sample_batch(batch_size)\n",
    "       \n",
    "        # Calculate iteration elements\n",
    "        g, eta_t = module.gradient(steps[-1], samples = ids), eta(t)\n",
    "        step = steps[-1] - eta_t * g\n",
    "\n",
    "        steps.append(step)\n",
    "        delta = np.sum((steps[-1] - steps[-2])**2)**.5\n",
    "        t += 1\n",
    "        \n",
    "    return np.array(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSS module consists of `evaluate`, `gradient` and `sample_batch` functions. To enbable the SGD descent behave like the GD, in the case a batch size is not passed the `sample_batch` returns $0,1,\\ldots,n\\_samples$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS:\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "    def evaluate(self, w: np.ndarray, samples: np.ndarray = None):\n",
    "        if samples is None:\n",
    "            samples = np.arange(self.X.shape[0])\n",
    "        \n",
    "        X, y = self.X[samples, :], self.y[samples]\n",
    "        \n",
    "        Y = np.broadcast_to(y[..., np.newaxis], (y.shape[0], w.shape[0]))\n",
    "        return np.sum( (X @ w.T - Y)**2, axis=0)\n",
    "    \n",
    "    def gradient(self, w: np.ndarray, samples: np.ndarray = None):\n",
    "        if samples is None:\n",
    "            samples = np.arange(self.X.shape[0])\n",
    "        return 2 * self.X[samples,:].T @ (self.X[samples,:] @ w.T - self.y[samples])\n",
    "    \n",
    "    def sample_batch(self, n:int=None):\n",
    "        if n is None:\n",
    "            return np.arange(self.X.shape[0])\n",
    "        return np.random.randint(self.X.shape[0], size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data according to the linear regression with Gaussian noise assumptions\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "w = np.array([5,-2])\n",
    "X = np.c_[np.random.uniform(low=-3, high=3, size=(n, 1)), np.ones((n, 1))]\n",
    "y = X @ w + np.random.normal(0, 5, size=(n,))\n",
    "\n",
    "module = RSS(X, y)\n",
    "vals = np.linspace(-30, 30, 100)\n",
    "contour = function_contour(module.evaluate, vals)\n",
    "\n",
    "eta, init = lambda t: .001, np.array([-20,-20])\n",
    "gd_steps = VanillaStochasticGradientDescent(module, eta=eta, init=init, batch_size=None)\n",
    "sgd_steps = VanillaStochasticGradientDescent(module, eta=eta, init=init, batch_size=3)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles = (r\"$\\text{Gradient Descent}$\", \n",
    "                                      r\"$\\text{Stochastic Gradient Descent}$\"))\\\n",
    "    .add_traces([go.Scatter(x=gd_steps[:,0], y=gd_steps[:,1], mode = \"markers+lines\", showlegend=False, marker_color=\"black\"),\n",
    "                 go.Scatter(x=sgd_steps[:,0], y=sgd_steps[:,1], mode = \"markers+lines\", showlegend=False, marker_color=\"black\"),\n",
    "                 contour,contour], rows=[1]*4, cols=[1,2,1,2])\\\n",
    "    .update_xaxes(range=[vals[0],vals[-1]])\\\n",
    "    .update_yaxes(range=[vals[0],vals[-1]])\\\n",
    "    .update_layout(width=800, height=400)\n",
    "\n",
    "fig.write_image(f\"../figures/rss_gd_sgd.png\")\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
